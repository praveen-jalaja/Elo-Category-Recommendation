{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deploymodels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kijCpPF-9Uok",
        "outputId": "22150080-4e1c-4f7c-e55a-24c5327db6e4"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHI4MgYVFNXE"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\"\"\"Copy of Final_submission.ipynb\r\n",
        "\r\n",
        "Automatically generated by Colaboratory.\r\n",
        "\r\n",
        "Original file is located at\r\n",
        "    https://colab.research.google.com/drive/1e_GJ6VV05t_Au8GhXsKJWZVLNJmaoLD4\r\n",
        "\r\n",
        "## Import libraries\r\n",
        "\"\"\"\r\n",
        "from flask_ngrok import run_with_ngrok\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import json\r\n",
        "import zipfile\r\n",
        "import gc\r\n",
        "import matplotlib.pylab as plt\r\n",
        "import seaborn as sns\r\n",
        "import warnings \r\n",
        "import datetime\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "sns.set_style(\"whitegrid\")\r\n",
        "from tqdm import tqdm\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.neighbors import KNeighborsRegressor\r\n",
        "import pickle\r\n",
        "from functools import reduce\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "import lightgbm\r\n",
        "import pickle\r\n",
        "import xgboost as xgb\r\n",
        "from scipy.stats import randint as sp_randint\r\n",
        "import flask\r\n",
        "from flask import Flask, jsonify, request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgq8LNmiAbRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95fd0cc-3809-424c-b21e-7af9ff05664f"
      },
      "source": [
        "## creating a dictionary with username and key\r\n",
        "api_token = {\"username\":\"praveenjalaja\",\"key\":\"254009cd9fcebc728fb2b82a4f00d301\"}\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "## new json created for download the data \r\n",
        "with open('kaggle.json', 'w') as file:\r\n",
        "    json.dump(api_token, file)\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "!ls ~/.kaggle\r\n",
        "!chmod 600 /root/.kaggle/kaggle.json\r\n",
        "\r\n",
        "## download the neccesary data\r\n",
        "!kaggle competitions download -c elo-merchant-category-recommendation\r\n",
        "\r\n",
        "! unzip /content/historical_transactions.csv.zip\r\n",
        "!rm /content/historical_transactions.csv.zip\r\n",
        "! unzip /content/new_merchant_transactions.csv.zip\r\n",
        "!rm /content/new_merchant_transactions.csv.zip\r\n",
        "! unzip /content/merchants.csv.zip\r\n",
        "!rm /content/merchants.csv.zip\r\n",
        "! unzip /content/train.csv.zip\r\n",
        "!rm /content/train.csv.zip\r\n",
        "! unzip /content/test.csv.zip\r\n",
        "!rm /content/test.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading Data%20Dictionary.xlsx to /content\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 14.0MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/3.02M [00:00<?, ?B/s]\n",
            "100% 3.02M/3.02M [00:00<00:00, 98.3MB/s]\n",
            "Downloading historical_transactions.csv.zip to /content\n",
            " 98% 537M/548M [00:11<00:00, 53.5MB/s]\n",
            "100% 548M/548M [00:11<00:00, 51.2MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/846k [00:00<?, ?B/s]\n",
            "100% 846k/846k [00:00<00:00, 108MB/s]\n",
            "Downloading Data_Dictionary.xlsx to /content\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 14.2MB/s]\n",
            "Downloading merchants.csv.zip to /content\n",
            " 71% 9.00M/12.7M [00:00<00:00, 17.6MB/s]\n",
            "100% 12.7M/12.7M [00:00<00:00, 21.5MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 36.3MB/s]\n",
            "Downloading new_merchant_transactions.csv.zip to /content\n",
            " 95% 47.0M/49.4M [00:01<00:00, 16.4MB/s]\n",
            "100% 49.4M/49.4M [00:01<00:00, 37.8MB/s]\n",
            "Archive:  /content/historical_transactions.csv.zip\n",
            "  inflating: historical_transactions.csv  \n",
            "Archive:  /content/new_merchant_transactions.csv.zip\n",
            "  inflating: new_merchant_transactions.csv  \n",
            "Archive:  /content/merchants.csv.zip\n",
            "  inflating: merchants.csv           \n",
            "Archive:  /content/train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  /content/test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ0IG4cHWc6B"
      },
      "source": [
        "master_path = \"/content/drive/MyDrive/Elo_kaggle/\"\r\n",
        "\r\n",
        "def category_3_encode(df):\r\n",
        "\r\n",
        "    ## label encode the category 3 variables.\r\n",
        "  d = {'A':1, 'B':2, 'C':3}\r\n",
        "  df['category_3']= df['category_3'].map(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeVFF00KFIqE"
      },
      "source": [
        "def loadData():\r\n",
        "\r\n",
        "\r\n",
        "  train_df                   = pd.read_csv('train.csv', parse_dates=[\"first_active_month\"])\r\n",
        "  test_df = pd.read_csv('test.csv', parse_dates=[\"first_active_month\"])\r\n",
        "  historical_transactions_df   = pd.read_csv('historical_transactions.csv',parse_dates=['purchase_date'])\r\n",
        "  category_3_encode(historical_transactions_df)\r\n",
        "  ###merchants_df                 = pd.read_csv('/content/merchants.csv')\r\n",
        "  new_merchant_transactions_df = pd.read_csv('new_merchant_transactions.csv',parse_dates=[\"purchase_date\"])\r\n",
        "  category_3_encode(new_merchant_transactions_df)\r\n",
        "  ##historical_transactions_df = reduce_memory_usage(historical_transactions_df)\r\n",
        "  ##new_merchant_transactions_df = reduce_memory_usage(new_merchant_transactions_df)\r\n",
        "  return train_df,historical_transactions_df,new_merchant_transactions_df,test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kn2Uqs1E7hv"
      },
      "source": [
        "train_df,historical_transactions_df,new_merchant_transactions_df,test_df = loadData()\r\n",
        "target  = train_df[['card_id','target']]\r\n",
        "target.set_index('card_id', inplace =True)\r\n",
        "cards = pd.concat([train_df.drop(['target'] , axis= 1) , test_df] , axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxMDPLCmV80_"
      },
      "source": [
        "cards.to_csv('cards.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqHEfV94W5LR",
        "outputId": "62a95c7e-79b8-4bf6-80bf-0d019983effd"
      },
      "source": [
        "new_merchant_transactions_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['authorized_flag', 'card_id', 'city_id', 'category_1', 'installments',\n",
              "       'category_3', 'merchant_category_id', 'merchant_id', 'month_lag',\n",
              "       'purchase_amount', 'purchase_date', 'category_2', 'state_id',\n",
              "       'subsector_id'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "4ywTIWckYKR2",
        "outputId": "f111300e-1ced-453a-c37f-12bf1fd7b9cb"
      },
      "source": [
        "new_merchant_transactions_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authorized_flag</th>\n",
              "      <th>card_id</th>\n",
              "      <th>city_id</th>\n",
              "      <th>category_1</th>\n",
              "      <th>installments</th>\n",
              "      <th>category_3</th>\n",
              "      <th>merchant_category_id</th>\n",
              "      <th>merchant_id</th>\n",
              "      <th>month_lag</th>\n",
              "      <th>purchase_amount</th>\n",
              "      <th>purchase_date</th>\n",
              "      <th>category_2</th>\n",
              "      <th>state_id</th>\n",
              "      <th>subsector_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_415bb3a509</td>\n",
              "      <td>107</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>307</td>\n",
              "      <td>M_ID_b0c793002c</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.557574</td>\n",
              "      <td>2018-03-11 14:57:36</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_415bb3a509</td>\n",
              "      <td>140</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>307</td>\n",
              "      <td>M_ID_88920c89e8</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.569580</td>\n",
              "      <td>2018-03-19 18:53:37</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_415bb3a509</td>\n",
              "      <td>330</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>507</td>\n",
              "      <td>M_ID_ad5237ef6b</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.551037</td>\n",
              "      <td>2018-04-26 14:08:44</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_415bb3a509</td>\n",
              "      <td>-1</td>\n",
              "      <td>Y</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>661</td>\n",
              "      <td>M_ID_9e84cda3b1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.671925</td>\n",
              "      <td>2018-03-07 09:43:21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_ef55cf8d4b</td>\n",
              "      <td>-1</td>\n",
              "      <td>Y</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>166</td>\n",
              "      <td>M_ID_3c86fa3831</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.659904</td>\n",
              "      <td>2018-03-22 21:07:53</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963026</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_1320dee851</td>\n",
              "      <td>142</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>309</td>\n",
              "      <td>M_ID_7754b67f3b</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.701828</td>\n",
              "      <td>2018-04-06 14:36:52</td>\n",
              "      <td>3.0</td>\n",
              "      <td>19</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963027</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_f112aa3381</td>\n",
              "      <td>158</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>560</td>\n",
              "      <td>M_ID_da063195b7</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.694390</td>\n",
              "      <td>2018-03-07 13:19:18</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963028</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_bd97b86450</td>\n",
              "      <td>69</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_9a9ccb6544</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.621031</td>\n",
              "      <td>2018-03-05 12:04:56</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963029</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_c0513fd84f</td>\n",
              "      <td>130</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>367</td>\n",
              "      <td>M_ID_40c28d596f</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.656749</td>\n",
              "      <td>2018-03-09 14:47:05</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963030</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_a935410f8e</td>\n",
              "      <td>19</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_d855771cd9</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.739395</td>\n",
              "      <td>2018-04-11 07:59:46</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1963031 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        authorized_flag          card_id  ...  state_id subsector_id\n",
              "0                     Y  C_ID_415bb3a509  ...         9           19\n",
              "1                     Y  C_ID_415bb3a509  ...         9           19\n",
              "2                     Y  C_ID_415bb3a509  ...         9           14\n",
              "3                     Y  C_ID_415bb3a509  ...        -1            8\n",
              "4                     Y  C_ID_ef55cf8d4b  ...        -1           29\n",
              "...                 ...              ...  ...       ...          ...\n",
              "1963026               Y  C_ID_1320dee851  ...        19           21\n",
              "1963027               Y  C_ID_f112aa3381  ...        15           34\n",
              "1963028               Y  C_ID_bd97b86450  ...         9           37\n",
              "1963029               Y  C_ID_c0513fd84f  ...         7           16\n",
              "1963030               Y  C_ID_a935410f8e  ...         9           37\n",
              "\n",
              "[1963031 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf-ZxDqMYJHR"
      },
      "source": [
        "CREATE TABLE historical_transactions (authorized_flag CHAR, card_id CHAR, city_id VARCHAR(20), category_1 CHAR, installments INT,\r\n",
        "       category_3 FLOAT(3), merchant_category_id VARCHAR(20), merchant_id CHAR, month_lag INT,\r\n",
        "       purchase_amount FLOAT(20), purchase_date VARCHAR(255), category_2 FLOAT(3), state_id VARCHAR(20),\r\n",
        "       subsector_id VARCHAR(5));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VbayR2d9nI6"
      },
      "source": [
        "\r\n",
        "def aggregated_features(new_df, df, aggs, grpby, name='',  prefix='', use_col=False):\r\n",
        "  '''\r\n",
        "  This function is to find the \r\n",
        "  aggregated values (min,max,mean,sum,nunique,std) for a columns aggregated by the groupby operation\r\n",
        "  \r\n",
        "  Parameters:\r\n",
        "  new_df   - features will be added to this DF\r\n",
        "  df       - original DF from which the features will be created\r\n",
        "  grpby    - based on this column we'll to group by\r\n",
        "  name     - name for the new features created\r\n",
        "  aggs     - dictionary contains key as the column the operation performed and list of operations as the value.\r\n",
        "  prefix   - added to the name of the feature -- default value empty\r\n",
        "  use_col  - if set True then the original column name will be uesd to name the new feature -- default value False\r\n",
        "  '''\r\n",
        "  ## boolean for using the original column name in the aggregated features\r\n",
        "  ## iterating through the columns of the need to be aggregated \r\n",
        "\r\n",
        "  for col, funcs in aggs.items():\r\n",
        "    for func in funcs:\r\n",
        "        # Getting the name of aggregation function\r\n",
        "        if isinstance(func, str):\r\n",
        "            func_str = func\r\n",
        "        else:\r\n",
        "            func_str = func.__name__ \r\n",
        "        # create the column\r\n",
        "        if use_col:\r\n",
        "          name = prefix+'_'+col+'_'+'{}'.format(func_str)\r\n",
        "\r\n",
        "        new_df[name] = df.groupby([grpby])[col].agg(func).values\r\n",
        "\r\n",
        "  return new_df\r\n",
        "\r\n",
        "def label_encoder(df, cols):\r\n",
        "  '''\r\n",
        "  This Function label encode the values in the specified columns and \r\n",
        "  return the data frame\r\n",
        "  Parameters:\r\n",
        "  df   - Original DataFrame\r\n",
        "  cols - label encode the specified columns\r\n",
        "  '''\r\n",
        "  lbl_enc = LabelEncoder()\r\n",
        "  for col in cols:\r\n",
        "    df[col] = lbl_enc.fit_transform(df[col].astype(str))\r\n",
        "  return df\r\n",
        "\r\n",
        "def encode_transactions(df):\r\n",
        "  '''This function is specially for encode the categorical values of \r\n",
        "  transactions data\r\n",
        "  parameters:\r\n",
        "  df: the Dataframe where the label encoding will performed on certain features\r\n",
        "  '''\r\n",
        "\r\n",
        "\r\n",
        "  #label encode the variables\r\n",
        "  df = label_encoder(df, ['authorized_flag','category_1'])\r\n",
        "  return df\r\n",
        "\r\n",
        "## Reference: https://medium.com/towards-artificial-intelligence/handling-missing-data-for-advanced-machine-learning-b6eb89050357\r\n",
        "\r\n",
        "def imputation_models(df, non_categorical,model_names, format):\r\n",
        "  '''This function is an universal function for imputation using \r\n",
        "  ML models\r\n",
        "  parameters:\r\n",
        "  df : The DataFrame\r\n",
        "  non_categorical : List of non_categorical columns names which can't be used for prediction model\r\n",
        "  nan_features: List of feature names which contains nan_values\r\n",
        "  models: dict , Key: feature name , value: Model\r\n",
        "  model_names: dict, key: feature name, value: name to be saved\r\n",
        "  filepath: file path where the models are saved.\r\n",
        "  format: format of model to be saved like '.sav'.\r\n",
        "  '''\r\n",
        "\r\n",
        "  if df[df.keys()].isna().any().any():\r\n",
        "    ## collecting all the non-categorical features \r\n",
        "    a = df[non_categorical]\r\n",
        "\r\n",
        "    ## dropping all the above collected features\r\n",
        "    df.drop(non_categorical, axis=1, inplace=True)\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    #select only columns which doesn't have any null values.\r\n",
        "    no_nan = [c for c in df.columns if c not in model_names.keys()]\r\n",
        "\r\n",
        "    for feat in model_names.keys():\r\n",
        "\r\n",
        "      if df[feat].isna().any().any():\r\n",
        "\r\n",
        "        #test set by selecting only rows which are having null values\r\n",
        "        df_null = df[df[feat].isna()]\r\n",
        "\r\n",
        "        #train set by selecting rows which doesn't have any null values\r\n",
        "        df_train = df.dropna()\r\n",
        "\r\n",
        "        ##after getting the required features, \r\n",
        "        ##we have to build a model with no-null observations to predict the null observstions.\r\n",
        "\r\n",
        "        file_directory = master_path+model_names[feat]+'.'+format\r\n",
        "\r\n",
        "        clf = pickle.load(open(file_directory, 'rb'))\r\n",
        "          \r\n",
        "\r\n",
        "        #make prediction only for the rows with null value\r\n",
        "        df.loc[df[feat].isna(), feat] = clf.predict(df_null[no_nan])\r\n",
        "\r\n",
        "    df[non_categorical] = a[non_categorical]\r\n",
        "    del a\r\n",
        "\r\n",
        "  return df\r\n",
        "\r\n",
        "def oneHotEncoding(df, features, original_df):\r\n",
        "  '''This function is for one-hot encoding the categorical features\r\n",
        "  parameters:\r\n",
        "  df: DataFrame\r\n",
        "  features: Features needs to be one hot encoded.'''\r\n",
        "\r\n",
        "  for feat in features:\r\n",
        "    unique_values = original_df[feat].unique()\r\n",
        "\r\n",
        "    for cat in unique_values:\r\n",
        "      df[feat+'={}'.format(cat)] = (df[feat] == cat).astype(int)\r\n",
        "\r\n",
        "def get_monthlag_stat(new_df, df, grpby, op, col, name, prefix=''):\r\n",
        "  \r\n",
        "  '''\r\n",
        "  Thid function is group by the the specified column and find the count or sum depending on the input.\r\n",
        "  Then perform basic operations like std, min, max etcetera\r\n",
        "  parameters\r\n",
        "  new_df - new features will be added to this DF\r\n",
        "  df     - original DF\r\n",
        "  grpby  - column using which we will group the data by\r\n",
        "  col    - operations will be performed on this column\r\n",
        "  name   - name for this columnn\r\n",
        "  prefix - prefix to the column name\r\n",
        "  '''\r\n",
        "  if op == 'sum':\r\n",
        "    tmp = df.groupby(grpby)[col].sum().unstack()\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[0]] = tmp.reset_index().iloc[:, -1].values\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[1]] = tmp.reset_index().iloc[:, -2].values\r\n",
        "  \r\n",
        "  if op == 'count':\r\n",
        "    tmp = df.groupby(grpby)[col].count().unstack()\r\n",
        "    # check if there is any null value and fill it with 0\r\n",
        "    if tmp.isna().sum().any() > 0:\r\n",
        "      tmp = tmp.fillna(0.0)\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[0]] = tmp.reset_index().iloc[:, 1:].std(axis=1).values\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[1]] = tmp.reset_index().iloc[:, 1:].max(axis=1).values\r\n",
        "    \r\n",
        "  return new_df\r\n",
        "\r\n",
        "#https://www.kaggle.com/fabiendaniel/elo-world?scriptVersionId=8335387\r\n",
        "def successive_aggregates(df, field1, field2):\r\n",
        "  '''\r\n",
        "  This function does is that it group the data twice and find\r\n",
        "  basic aggregate values.\r\n",
        "  First it will goup by card_id and all the specified column one by one.\r\n",
        "  Then it will find the agg values like mean, min, max and std\r\n",
        "  for the purchase amount for each group.\r\n",
        "  Parameters:\r\n",
        "  df      - original DataFrame\r\n",
        "  field1  - first groupby along with card_id\r\n",
        "  field2  - second grouby along with card_id\r\n",
        "  '''\r\n",
        "  t = df.groupby(['card_id', field1])[field2].mean()\r\n",
        "  u = pd.DataFrame(t).reset_index().groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std'])\r\n",
        "  u.columns = [field1 + '_' + field2 + '_' + col for col in u.columns.values]\r\n",
        "  u.reset_index(inplace=True)\r\n",
        "  return u\r\n",
        "\r\n",
        "\r\n",
        "def successive_aggregation(new_df, df, field1='', field2 = '', other_columns=[]):\r\n",
        "  '''\r\n",
        "  This function is aggregates transactions columns successively with the columns mentioned.\r\n",
        "\r\n",
        "  parameters:\r\n",
        "\r\n",
        "  new_df  - The DataFrame which need to merged at end.\r\n",
        "  df      - The DataFrame with all the column required.\r\n",
        "  field1  - This is first field of many successive fields.\r\n",
        "  field2 - The second field for the aggregation, which constant for which te aggregation done with other features.\r\n",
        "  other_columns - all the other columns which will be later aggregated as the field_1.\r\n",
        "  '''\r\n",
        "\r\n",
        "  ## temp df for storing the aggregations \r\n",
        "  succ_agg = successive_aggregates(df, field1= field1, field2= field2)\r\n",
        "  ## creating other successive features \r\n",
        "  for  col in other_columns:\r\n",
        "    succ_agg = succ_agg.merge(successive_aggregates(df, col, field2),on=['card_id'], how='left')\r\n",
        "  \r\n",
        "  ## merge with the new df finally\r\n",
        "  new_df = new_df.merge(succ_agg, on = ['card_id'], how = 'left')\r\n",
        "\r\n",
        "  return new_df\r\n",
        "\r\n",
        "def getdatefeatures(df):\r\n",
        "  '''This function is to get all the numerical \r\n",
        "  features from the purchase Date feature\r\n",
        "  Parameters:\r\n",
        "  df: The dataFrame'''\r\n",
        "\r\n",
        "  df['week'] = df['purchase_date'].dt.week.values\r\n",
        "  df['dayofweek'] = df['purchase_date'].dt.dayofweek.values\r\n",
        "  df['hour'] = df['purchase_date'].dt.hour.values\r\n",
        "\r\n",
        "def timebtwpurchases(df, groupby, column, shift = 1):\r\n",
        "\r\n",
        "  ''' This function is to extract the shifted columns between the purchases.\r\n",
        "\r\n",
        "  parameters:\r\n",
        "\r\n",
        "  df - The DataFrame\r\n",
        "  groupby - The col by which the data is grouped by\r\n",
        "  column - column for which the shifted features are calcuated\r\n",
        "  shift - how many time periods shift is required\r\n",
        "  '''\r\n",
        "\r\n",
        "\r\n",
        "  ## first to sort values by the column for which shifting needed.\r\n",
        "\r\n",
        "  df = df.sort_values(column)\r\n",
        "\r\n",
        "  for i in range(shift):\r\n",
        "    ## creating the shited time between two purchases in a group\r\n",
        "    df['prev_{}_'.format(i+1)+column] = df.groupby([groupby])[column].shift(i+1)\r\n",
        "    ## calcuating the shift in different time representations like days,seconds,hours\r\n",
        "    df['purchase_date_diff_{}_days'.format(i+1)] = (df[column] - df['prev_{}_'.format(i+1)+column]).dt.days.values\r\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] = df['purchase_date_diff_{}_days'.format(i+1)].values * 24 * 3600\r\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] += (df[column] - df['prev_{}_'.format(i+1)+column]).dt.seconds.values\r\n",
        "    df['purchase_date_diff_{}_hours'.format(i+1)] = df.iloc[:, -1].values // 3600\r\n",
        "\r\n",
        "  return df\r\n",
        "\r\n",
        "## reference: https://towardsdatascience.com/find-your-best-customers-with-customer-segmentation-in-python-61d602f9eee6\r\n",
        "def RScore(x,p,d):\r\n",
        "    if x <= d[p][0.011]:\r\n",
        "        return 1\r\n",
        "    elif x <= d[p][0.050]:\r\n",
        "        return 2\r\n",
        "    elif x <= d[p][0.25]: \r\n",
        "        return 3\r\n",
        "    elif x <= d[p][0.5]:\r\n",
        "        return 4\r\n",
        "    elif x <= d[p][0.75]:\r\n",
        "        return 5\r\n",
        "    elif x <= d[p][0.95]:\r\n",
        "        return 6\r\n",
        "    elif x <= d[p][0.989]:\r\n",
        "        return 7\r\n",
        "    else:\r\n",
        "        return 8\r\n",
        "    \r\n",
        "def FMScore(x,p,d):\r\n",
        "    if x <= d[p][0.011]:\r\n",
        "        return 8\r\n",
        "    elif x <= d[p][0.050]:\r\n",
        "        return 7\r\n",
        "    elif x <= d[p][0.25]: \r\n",
        "        return 6\r\n",
        "    elif x <= d[p][0.5]:\r\n",
        "        return 5\r\n",
        "    elif x <= d[p][0.75]:\r\n",
        "        return 4\r\n",
        "    elif x <= d[p][0.95]:\r\n",
        "        return 3\r\n",
        "    elif x <= d[p][0.989]:\r\n",
        "        return 2\r\n",
        "    else:\r\n",
        "        return 1\r\n",
        "\r\n",
        "def rfm(df,quantiles, transc):\r\n",
        "  '''This function is to calcualte RFM score and RFM index for the dataframe with RFM.\r\n",
        "\r\n",
        "  parameters:\r\n",
        "\r\n",
        "  df: The DataFrame\r\n",
        "  quantiles: qunatiles for the RFM score and index to calculated\r\n",
        "  transc : type of transactions(new or hist)\r\n",
        "\r\n",
        "  '''\r\n",
        "\r\n",
        "  ## grouping quantiles\r\n",
        "  df[transc+'r_quantile'] = df[transc+'purchase_recency'].apply(RScore, args=(transc+'purchase_recency',quantiles))\r\n",
        "  df[transc+'f_quantile'] = df[transc+'count'].apply(FMScore, args=(transc+'count',quantiles))\r\n",
        "  df[transc+'m_quantile'] = df[transc+'purchase_amount_sum'].apply(FMScore, args=(transc+'purchase_amount_sum',quantiles))\r\n",
        "  ## calaculating RFM index and RFMScore\r\n",
        "  df[transc+'RFMindex'] = df[transc+'r_quantile'].map(str)+df[transc+'f_quantile'].map(str)+df[transc+'m_quantile'].map(str)\r\n",
        "  df[transc+'RFMindex'] = df[transc+'RFMindex'].astype(int)                     \r\n",
        "  df[transc+'RFMScore'] = df[transc+'r_quantile']+df[transc+'f_quantile']+df[transc+'m_quantile']\r\n",
        "\r\n",
        "holidays = {'EasterDay_2017' : '2017-04-16',\r\n",
        "          'AllSoulsDay_2017': '2017-11-2',\r\n",
        "          'ChristmasDay_2017': '2017-12-25',\r\n",
        "          'FathersDay_2017': '2017-08-13',\r\n",
        "          'ChildrenDay_2017':'2017-10-12',\r\n",
        "          'BlackFriday_2017':'2017-11-24',\r\n",
        "          'ValentineDay_2017':'2017-06-12',\r\n",
        "          'MothersDay_2018':'2018-05-13'}\r\n",
        "\r\n",
        "def preprocess(new_merchant_transactions,historical_transactions):\r\n",
        "    \r\n",
        "\r\n",
        "  non_categorical= ['card_id', 'merchant_id', 'purchase_date']\r\n",
        "\r\n",
        "\r\n",
        "  model_names_transacations= {'category_2':'category_2_new_merchants_model',\r\n",
        "                        'category_3':'category_3_new_merchants_model' }\r\n",
        "\r\n",
        "\r\n",
        "  ## encoding the categorical features in new_merchants\r\n",
        "  new_merchant_transactions = encode_transactions(new_merchant_transactions)\r\n",
        "\r\n",
        "  new_merchant_transactions = imputation_models(df = new_merchant_transactions,\r\n",
        "                    non_categorical = non_categorical,\r\n",
        "                    model_names =model_names_transacations,\r\n",
        "                    format = '.sav')\r\n",
        "  \r\n",
        "\r\n",
        "  model_names_transacations= {'category_2':'category_2_historical_merchants_model',\r\n",
        "                      'category_3':'category_3_historical_merchants_model' }\r\n",
        "\r\n",
        "\r\n",
        "  ## encoding the categorical features in historical transactions\r\n",
        "  historical_transactions = encode_transactions(historical_transactions)\r\n",
        "\r\n",
        "  historical_transactions = imputation_models(df = historical_transactions,\r\n",
        "                    non_categorical = non_categorical,\r\n",
        "                    model_names =model_names_transacations,\r\n",
        "                    format = '.sav')\r\n",
        "  \r\n",
        "  \r\n",
        "  ## One-hot encoding the categorical features\r\n",
        "  categorical_features = ['category_2','category_3','month_lag']\r\n",
        "\r\n",
        "  ## one-hot encoding historical transactions \r\n",
        "  oneHotEncoding(historical_transactions, features=categorical_features,\r\n",
        "                original_df = historical_transactions_df)\r\n",
        "\r\n",
        "  ## one-hot encoding new merchants transactions\r\n",
        "  oneHotEncoding(new_merchant_transactions, features=categorical_features,\r\n",
        "                original_df = new_merchant_transactions_df)\r\n",
        "  \r\n",
        "\r\n",
        "  ## preprocess the purchase_amount\r\n",
        "  new_merchant_transactions['purchase_amount'] = np.round(new_merchant_transactions['purchase_amount'] / 0.00150265118 + 497.06, 2)\r\n",
        "  historical_transactions['purchase_amount'] = np.round(historical_transactions['purchase_amount'] / 0.00150265118 + 497.06, 2)\r\n",
        "\r\n",
        "  #is_weekend is a feature which purchase_date is weekend or weekday.\r\n",
        "  new_merchant_transactions['is_weekend'] = new_merchant_transactions['purchase_date'].dt.dayofweek\r\n",
        "  #>5 to check whether the day is sat or sunday then, if it is then assign a val 1 else 0\r\n",
        "  new_merchant_transactions['is_weekend'] = new_merchant_transactions['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\r\n",
        "  historical_transactions['is_weekend'] = historical_transactions['purchase_date'].dt.dayofweek\r\n",
        "  #>5 to check whether the day is sat or sunday then, if it is then assign a val 1 else 0\r\n",
        "  historical_transactions['is_weekend'] = historical_transactions['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\r\n",
        "\r\n",
        "  reference_date = '2018-12-31'\r\n",
        "  reference_date = pd.to_datetime(reference_date)\r\n",
        "  ## calcuating month difference \r\n",
        "  new_merchant_transactions['month_diff'] = (reference_date - new_merchant_transactions['purchase_date']).dt.days // (30 + new_merchant_transactions['month_lag'])\r\n",
        "  historical_transactions['month_diff'] = (reference_date - historical_transactions['purchase_date']).dt.days // (30 + historical_transactions['month_lag'])\r\n",
        "\r\n",
        "  new_merchant_transactions['amount_month_ratio'] = new_merchant_transactions['purchase_amount'].values / (1.0 + new_merchant_transactions['month_diff'].values)\r\n",
        "  historical_transactions['amount_month_ratio'] = historical_transactions['purchase_amount'].values / (1.0 + historical_transactions['month_diff'].values)\r\n",
        "\r\n",
        "  getdatefeatures(historical_transactions)\r\n",
        "  getdatefeatures(new_merchant_transactions)\r\n",
        "\r\n",
        "  new_merchant_transactions = timebtwpurchases(new_merchant_transactions, 'card_id', 'purchase_date', 2)\r\n",
        "\r\n",
        "\r\n",
        "  ## we are gonna represent number days as the feature. if the values is above 75 then it will become zero.\r\n",
        "  for day, date in holidays.items():\r\n",
        "    ## new_transactions \r\n",
        "    new_merchant_transactions[day] = (pd.to_datetime(date) - new_merchant_transactions['purchase_date']).dt.days\r\n",
        "    new_merchant_transactions[day] = new_merchant_transactions[day].apply(lambda x: x if x > 0 and x < 75 else 0)\r\n",
        "\r\n",
        "\r\n",
        "  return new_merchant_transactions,historical_transactions\r\n",
        "\r\n",
        "def engineered_features(new,ht):\r\n",
        "  \r\n",
        "  ## new_transactions\r\n",
        "  new_merch_features = pd.DataFrame(new.groupby(['card_id']).size()).reset_index()\r\n",
        "  new_merch_features.columns = ['card_id', 'new_transc_count']\r\n",
        "  ## historical_transactions \r\n",
        "  historical_trans_features = pd.DataFrame(ht.groupby(['card_id']).size()).reset_index()\r\n",
        "  historical_trans_features.columns = ['card_id', 'hist_transc_count']\r\n",
        "\r\n",
        "\r\n",
        "  ##unique id's in the transactions\r\n",
        "  aggs = {'city_id':['nunique'],\r\n",
        "        'state_id' :['nunique'],\r\n",
        "        'merchant_category_id':['nunique'],\r\n",
        "        'subsector_id':['nunique'],\r\n",
        "        'merchant_id':['nunique']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "  \r\n",
        "  ## categorty enigneered features\r\n",
        "  aggs = {'category_1':['sum', 'mean'],\r\n",
        "          'authorized_flag': ['sum', 'mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "    \r\n",
        "  ## category_1\r\n",
        "  new_merch_features['new_transc_category_1_sum_0'] = new_merch_features['new_transc_count'].values - \\\r\n",
        "  new_merch_features['new_transc_category_1_sum'].values\r\n",
        "  historical_trans_features['hist_transc_category_1_sum_0'] = historical_trans_features['hist_transc_count'].values - \\\r\n",
        "  historical_trans_features['hist_transc_category_1_sum'].values\r\n",
        "  ## authorized_flag\r\n",
        "  new_merch_features['new_transc_denied_count'] = new_merch_features['new_transc_count'].values - \\\r\n",
        "  new_merch_features['new_transc_authorized_flag_sum'].values\r\n",
        "  historical_trans_features['hist_transc_denied_count'] = historical_trans_features['hist_transc_count'].values - \\\r\n",
        "  historical_trans_features['hist_transc_authorized_flag_sum'].values\r\n",
        "\r\n",
        "\r\n",
        "  ### installment features \r\n",
        "  aggs = {'installments':['mean', 'sum', 'max', 'min', 'std', 'skew']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ### category_2 one-hot encoded features \r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'category_2=1.0':['sum', 'mean'],\r\n",
        "          'category_2=3.0':['sum', 'mean'],\r\n",
        "          'category_2=2.0':['sum', 'mean'],\r\n",
        "          'category_2=4.0':['sum', 'mean'],\r\n",
        "          'category_2=5.0':['sum', 'mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ### category_3 one-hot encoded features \r\n",
        "\r\n",
        "  aggs = {'category_3=2.0':['sum', 'mean'],\r\n",
        "          'category_3=1.0':['sum', 'mean'],\r\n",
        "          'category_3=3.0':['sum', 'mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  #find mean of the count of the transac for merchant id\r\n",
        "  historical_trans_features['hist_transc_merchant_id_count_mean'] = historical_trans_features['hist_transc_count'].values / (1.0+historical_trans_features['hist_transc_merchant_id_nunique'].values)\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features['new_transc_merchant_id_count_mean'] = new_merch_features['new_transc_count'].values/ (1.0+ new_merch_features['new_transc_merchant_id_nunique'].values)\r\n",
        "\r\n",
        "\r\n",
        "  grpby_lag = ['card_id', 'month_lag']\r\n",
        "  historical_trans_features = get_monthlag_stat(historical_trans_features, ht, grpby=grpby_lag, op='count',\r\n",
        "                                            col='purchase_amount', prefix='hist_transc_', name=['count_std','count_max'])\r\n",
        "\r\n",
        "  new_merch_features = get_monthlag_stat(new_merch_features, new, grpby=grpby_lag, op='count',\r\n",
        "                                        col='purchase_amount', prefix='new_transc_', name=['count_std','count_max'])\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'purchase_amount':['sum', 'mean', 'max', 'min', 'median', 'std', 'skew']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  ## difference in the amount spend with cards\r\n",
        "  new_merch_features['new_transc_amount_diff'] = new_merch_features['new_transc_purchase_amount_max'].values - new_merch_features['new_transc_purchase_amount_min'].values\r\n",
        "\r\n",
        "  historical_trans_features['hist_transc_amount_diff'] = historical_trans_features['hist_transc_purchase_amount_max'].values - historical_trans_features['hist_transc_purchase_amount_min'].values\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features = successive_aggregation(new_merch_features, new,\r\n",
        "                                              field1='category_1', field2 = 'purchase_amount',\r\n",
        "                                              other_columns = ['installments', 'city_id', \r\n",
        "                                                              'merchant_category_id', 'merchant_id',\r\n",
        "                                                              'subsector_id','category_2','category_3'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  historical_trans_features = successive_aggregation(historical_trans_features, ht,\r\n",
        "                                              field1='category_1', field2 = 'purchase_amount',\r\n",
        "                                              other_columns = ['installments', 'city_id', \r\n",
        "                                                              'merchant_category_id', 'merchant_id',\r\n",
        "                                                              'subsector_id','category_2','category_3'])\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'month_lag': ['nunique', 'mean', 'std', 'min', 'max', 'skew']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'purchase_date': ['max','min']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ## diff in purchase_date from max to min \r\n",
        "  new_merch_features['new_transc_purchase_date_diff'] = (new_merch_features['new_transc_purchase_date_max'] - new_merch_features['new_transc_purchase_date_min']).dt.days.values\r\n",
        "\r\n",
        "  ## purchase_count_ratio\r\n",
        "  new_merch_features['new_transc_purchase_count_ratio'] = new_merch_features['new_transc_count'].values / (1.0 + new_merch_features['new_transc_purchase_date_diff'].values)\r\n",
        "\r\n",
        "  ## diff in purchase_date from max to min \r\n",
        "  historical_trans_features['hist_transc_purchase_date_diff'] = (historical_trans_features['hist_transc_purchase_date_max'] - historical_trans_features['hist_transc_purchase_date_min']).dt.days.values\r\n",
        "\r\n",
        "  ## purchase_count_ratio\r\n",
        "  historical_trans_features['hist_transc_purchase_count_ratio'] = historical_trans_features['hist_transc_count'].values / (1.0 + historical_trans_features['hist_transc_purchase_date_diff'].values)\r\n",
        "\r\n",
        "\r\n",
        "  ## aggregate features for is_weekend \r\n",
        "  aggs = {'is_weekend': ['sum','mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'month_diff': ['mean', 'min', 'max']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  ## aggregated features on the amount ratio and month_lag.\r\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\r\n",
        "          'month_lag=1': ['sum','mean'],\r\n",
        "          'month_lag=2':['sum','mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\r\n",
        "          'month_lag=0': ['sum','mean'],\r\n",
        "          'month_lag=-1':['sum','mean'],\r\n",
        "          'month_lag=-2':['sum','mean']}\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ## month_lag ratio between two month_lags.\r\n",
        "  new_merch_features['new_transc_month_lag_1_2_ratio'] = new_merch_features['new_transc_month_lag=1_sum'] \\\r\n",
        "                                                                    / (1.0+ new_merch_features['new_transc_month_lag=2_sum'])\r\n",
        "\r\n",
        "\r\n",
        "  ## month_lag ratio in historical transactions\r\n",
        "\r\n",
        "  historical_trans_features['hist_transc_month_lag_0_-1_ratio'] = historical_trans_features['hist_transc_month_lag=0_sum'] \\\r\n",
        "                                                                    / (1.0+ historical_trans_features['hist_transc_month_lag=-1_sum'])\r\n",
        "\r\n",
        "  historical_trans_features['hist_transc_month_lag_0_-2_ratio'] = historical_trans_features['hist_transc_month_lag=0_sum'] \\\r\n",
        "                                                                    / (1.0+ historical_trans_features['hist_transc_month_lag=-2_sum'])\r\n",
        "  tmp = historical_trans_features[['hist_transc_month_lag=0_sum','hist_transc_month_lag=-1_sum','hist_transc_month_lag=-2_sum']].sum(axis=1)\r\n",
        "\r\n",
        "  ## ratio of the summed month lags with the transaction\r\n",
        "  historical_trans_features['hist_transc_month_lag_sum_ratio'] = tmp / (1.0+ historical_trans_features['hist_transc_count'])\r\n",
        "\r\n",
        "\r\n",
        "  ## aggregated features on  day, hour , week\r\n",
        "  aggs = {'week': ['nunique', 'mean', 'min', 'max'],\r\n",
        "          'dayofweek': ['nunique', 'mean', 'min', 'max'],\r\n",
        "          'hour':['nunique', 'mean', 'min', 'max']}\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  ## aggregation difference in time features\r\n",
        "  aggs = {'purchase_date_diff_1_seconds': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_1_days': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_1_hours': ['mean', 'std', 'max', 'min']}\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  ## aggregation difference in time features\r\n",
        "  aggs = {'purchase_date_diff_2_seconds': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_2_days': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_2_hours': ['mean', 'std', 'max', 'min']}\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "    ## aggregation of holidays\r\n",
        "  aggs = dict(zip(holidays.keys(),[['mean'] for x in holidays.keys()]))\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  return new_merch_features,historical_trans_features\r\n",
        "\r\n",
        "def preprocess_train(df,new_merch_features,historical_trans_features):\r\n",
        "\r\n",
        "  df = reduce(lambda left,right: pd.merge(left,right,on='card_id', how='left'), [df,new_merch_features,\r\n",
        "                                                                                 historical_trans_features])\r\n",
        "  \r\n",
        "  reference_date = pd.to_datetime('2018-12-31')\r\n",
        "  df['year'] = df['first_active_month'].dt.year.values\r\n",
        "  df['month'] = df['first_active_month'].dt.month.values\r\n",
        "  ## extracting elapsed dates \r\n",
        "  df['hist_transc_no_of_days'] = ( pd.to_datetime(df['hist_transc_purchase_date_max']) -  pd.to_datetime(df['hist_transc_purchase_date_max'])).dt.days\r\n",
        "  df['new_transc_no_of_days'] = (pd.to_datetime(df['new_transc_purchase_date_max']) - pd.to_datetime(df['new_transc_purchase_date_max'])).dt.days\r\n",
        "  ## recency of the puchases in terms of fractions\r\n",
        "  df['hist_transc_purchase_active_diff'] = (pd.to_datetime(df['hist_transc_purchase_date_min'].astype(str).apply(lambda x: x[:7])) - df['first_active_month']).dt.days.values\r\n",
        "  df['hist_transc_purchase_recency'] = (reference_date - pd.to_datetime(df['hist_transc_purchase_date_max']))/(24*np.timedelta64(1, 'h'))\r\n",
        "  df['new_transc_purchase_recency'] = (reference_date - pd.to_datetime(df['new_transc_purchase_date_max']))/(24*np.timedelta64(1, 'h')) \r\n",
        "\r\n",
        "  df = label_encoder(df, cols = ['month','year'])\r\n",
        "\r\n",
        "  quantiles_new = df[['new_transc_purchase_recency','new_transc_count','new_transc_purchase_amount_sum']].quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989]).to_dict()\r\n",
        "  ## quantiles of RFM with historical transactions\r\n",
        "  quantiles_hist = df[['hist_transc_purchase_recency','hist_transc_count','hist_transc_purchase_amount_sum']].quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989]).to_dict()\r\n",
        "\r\n",
        "  rfm(df,quantiles_new,transc = 'new_transc_')\r\n",
        "  rfm(df,quantiles_hist,transc = 'hist_transc_')\r\n",
        "\r\n",
        "  remove_cols = ['first_active_month','new_transc_purchase_date_max',\r\n",
        " 'new_transc_purchase_date_min','hist_transc_purchase_date_max',\r\n",
        " 'hist_transc_purchase_date_min']\r\n",
        "\r\n",
        "  df = df.drop(labels=remove_cols, axis = 1)\r\n",
        "\r\n",
        "  return df\r\n",
        "\r\n",
        "app = Flask(__name__)\r\n",
        "run_with_ngrok(app)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "@app.route('/loyalty_score')\r\n",
        "def loyalty_score():\r\n",
        "  return flask.render_template('loyalty_score.html')\r\n",
        "\r\n",
        "@app.route('/predict',methods = ['POST'] )\r\n",
        "def predict_loyalty_score():\r\n",
        "\r\n",
        "  ''' This function is predict the given card_id's predicted loyalty Score\r\n",
        "\r\n",
        "  parameters:\r\n",
        "  X : List of the card_id's\r\n",
        "\r\n",
        "  returns Dataframe with predicted loyalty score for each card_id.'''\r\n",
        "  \r\n",
        "\r\n",
        "  card_id = request.form.to_dict()\r\n",
        "  X = card_id['card_id']\r\n",
        "\r\n",
        "  X = [str(X)]\r\n",
        "  ###X = ['C_ID_aef4b7218b']\r\n",
        "  print(\"Fetching the transactional and card_id data\")\r\n",
        "  train = cards.loc[cards['card_id'].isin(X)]\r\n",
        "\r\n",
        "  historical_transactions = historical_transactions_df[historical_transactions_df['card_id'].isin(X)]\r\n",
        "  new_merchant_transactions = new_merchant_transactions_df[new_merchant_transactions_df['card_id'].isin(X)]\r\n",
        "  print(\"PreProcess the transactions data......\")\r\n",
        "  new_merchant_transactions,historical_transactions  = preprocess(new_merchant_transactions,historical_transactions)\r\n",
        "  print(\"Feature Engineering the transactions data.....\")\r\n",
        "  new_merch_features,historical_trans_features = engineered_features(new_merchant_transactions,historical_transactions)\r\n",
        "  print('preprocess the Feature Engineered Data')\r\n",
        "  train = preprocess_train(train,new_merch_features,historical_trans_features)\r\n",
        "\r\n",
        "  train.set_index('card_id',inplace=True)\r\n",
        "\r\n",
        "  print(\"Predicting the Loyalty Score.....\")\r\n",
        "  with open(master_path+'lgbm_final.sav', 'rb') as pickle_file:\r\n",
        "    mod = pickle.load(pickle_file)\r\n",
        "  pred_lgb = mod.predict(train , num_iteration=mod.best_iteration)\r\n",
        "\r\n",
        "  with open(master_path+ 'xgboost_final.sav', 'rb') as pickle_file:\r\n",
        "      mod = pickle.load(pickle_file)\r\n",
        "  pred_xgb = mod.predict(xgb.DMatrix(train[mod.feature_names]), ntree_limit=mod.best_ntree_limit+50)\r\n",
        "\r\n",
        "  meta_data = np.vstack([pred_xgb, pred_lgb]).transpose()\r\n",
        "\r\n",
        "  with open(master_path+'stacked_final.sav', 'rb') as pickle_file:\r\n",
        "      mod = pickle.load(pickle_file)\r\n",
        "  pred_stack = mod.predict(meta_data)\r\n",
        "\r\n",
        "  predcited_target = pd.DataFrame()\r\n",
        "\r\n",
        "  predcited_target['card_id'] = train.index \r\n",
        "\r\n",
        "  predcited_target['Loyalty_Score'] = pred_stack\r\n",
        "\r\n",
        "  ##predcited_target.set_index('card_id',inplace=True)\r\n",
        "\r\n",
        "  return flask.render_template('simple.html',  tables=[predcited_target.to_html(classes='data')], titles=predcited_target.columns.values)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9sB6NO-DomU",
        "outputId": "e5fae410-f07c-4fef-fc80-15ec574a5b88"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://3c5e51a2be49.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Feb/2021 19:17:16] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [09/Feb/2021 19:17:17] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [09/Feb/2021 19:17:28] \"\u001b[37mGET /loyalty_score HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            "Feature Engineering the transactions data.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Feb/2021 19:17:40] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Feb/2021 19:17:40] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Feb/2021 19:18:05] \"\u001b[31m\u001b[1mGET /predict HTTP/1.1\u001b[0m\" 405 -\n",
            "127.0.0.1 - - [09/Feb/2021 19:18:21] \"\u001b[37mGET /loyalty_score HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Feb/2021 19:18:46] \"\u001b[37mGET /loyalty_score HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            "Feature Engineering the transactions data.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Feb/2021 19:19:01] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}