{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx754_dtkHVT"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XvJdzKfjV8D"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import json\r\n",
        "import zipfile\r\n",
        "import gc\r\n",
        "import matplotlib.pylab as plt\r\n",
        "import seaborn as sns\r\n",
        "import warnings \r\n",
        "import datetime\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "sns.set_style(\"whitegrid\")\r\n",
        "from tqdm import tqdm\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.neighbors import KNeighborsRegressor\r\n",
        "import pickle\r\n",
        "from prettytable import PrettyTable\r\n",
        "import prettytable\r\n",
        "from functools import reduce\r\n",
        "from sklearn.metrics import make_scorer\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from IPython.display import Image\r\n",
        "import lightgbm\r\n",
        "import pickle\r\n",
        "import xgboost as xgb\r\n",
        "from scipy.stats import randint as sp_randint\r\n",
        "\r\n",
        "master_path = '/content/drive/MyDrive/Elo_kaggle/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0f3c3c5kR8H"
      },
      "source": [
        "## Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-boDQiiqpX2l"
      },
      "source": [
        "## Reference: https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655\r\n",
        "\r\n",
        "def reduce_memory_usage(df, verbose=True):\r\n",
        "  '''\r\n",
        "  This function reduces the memory sizes of datafram by changing the dattypes of the columns.\r\n",
        "  Parameters\r\n",
        "  df - DataFrame whose size to be reduced\r\n",
        "  verbose - Boolean, to mention the verbose required or not.\r\n",
        "  '''\r\n",
        "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n",
        "  start_mem = df.memory_usage().sum() / 1024**2\r\n",
        "  for col in df.columns:\r\n",
        "      col_type = df[col].dtypes\r\n",
        "      if col_type in numerics:\r\n",
        "          c_min = df[col].min()\r\n",
        "          c_max = df[col].max()\r\n",
        "          if str(col_type)[:3] == 'int':\r\n",
        "              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n",
        "                  df[col] = df[col].astype(np.int8)\r\n",
        "              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n",
        "                  df[col] = df[col].astype(np.int16)\r\n",
        "              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n",
        "                  df[col] = df[col].astype(np.int32)\r\n",
        "              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n",
        "                  df[col] = df[col].astype(np.int64)\r\n",
        "          else:\r\n",
        "              c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\r\n",
        "              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\r\n",
        "                  df[col] = df[col].astype(np.float16)\r\n",
        "              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\r\n",
        "                  df[col] = df[col].astype(np.float32)\r\n",
        "              else:\r\n",
        "                  df[col] = df[col].astype(np.float64)\r\n",
        "  end_mem = df.memory_usage().sum() / 1024**2\r\n",
        "  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n",
        "  return df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj-XdQe4pcF1"
      },
      "source": [
        "def downloadData():\r\n",
        "  ## creating a dictionary with username and key\r\n",
        "  api_token = {\"username\":\"praveenjalaja\",\"key\":\"254009cd9fcebc728fb2b82a4f00d301\"}\r\n",
        "  !mkdir -p ~/.kaggle\r\n",
        "  ## new json created for download the data \r\n",
        "  with open('kaggle.json', 'w') as file:\r\n",
        "      json.dump(api_token, file)\r\n",
        "  !cp kaggle.json ~/.kaggle/\r\n",
        "  !ls ~/.kaggle\r\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\r\n",
        "\r\n",
        "  ## download the neccesary data\r\n",
        "  !kaggle competitions download -c elo-merchant-category-recommendation\r\n",
        "\r\n",
        "  ! unzip /content/historical_transactions.csv.zip\r\n",
        "  !rm /content/historical_transactions.csv.zip\r\n",
        "  ! unzip /content/new_merchant_transactions.csv.zip\r\n",
        "  !rm /content/new_merchant_transactions.csv.zip\r\n",
        "  ! unzip /content/merchants.csv.zip\r\n",
        "  !rm /content/merchants.csv.zip\r\n",
        "  ! unzip /content/train.csv.zip\r\n",
        "  !rm /content/train.csv.zip\r\n",
        "  ! unzip /content/test.csv.zip\r\n",
        "  !rm /content/test.csv.zip\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNMvciOSls9J"
      },
      "source": [
        "def loadData():\r\n",
        "\r\n",
        "\r\n",
        "  train_df                   = pd.read_csv('/content/train.csv', parse_dates=[\"first_active_month\"])\r\n",
        "  test_df = pd.read_csv('/content/test.csv', parse_dates=[\"first_active_month\"])\r\n",
        "  historical_transactions_df   = pd.read_csv('/content/historical_transactions.csv',parse_dates=['purchase_date'])\r\n",
        "  category_3_encode(historical_transactions_df)\r\n",
        "  ###merchants_df                 = pd.read_csv('/content/merchants.csv')\r\n",
        "  new_merchant_transactions_df = pd.read_csv('/content/new_merchant_transactions.csv',parse_dates=[\"purchase_date\"])\r\n",
        "  category_3_encode(new_merchant_transactions_df)\r\n",
        "  ##historical_transactions_df = reduce_memory_usage(historical_transactions_df)\r\n",
        "  ##new_merchant_transactions_df = reduce_memory_usage(new_merchant_transactions_df)\r\n",
        "  return train_df,historical_transactions_df,new_merchant_transactions_df,test_df\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQM865aVkV92"
      },
      "source": [
        "def aggregated_features(new_df, df, aggs, grpby, name='',  prefix='', use_col=False):\r\n",
        "  '''\r\n",
        "  This function is to find the \r\n",
        "  aggregated values (min,max,mean,sum,nunique,std) for a columns aggregated by the groupby operation\r\n",
        "  \r\n",
        "  Parameters:\r\n",
        "  new_df   - features will be added to this DF\r\n",
        "  df       - original DF from which the features will be created\r\n",
        "  grpby    - based on this column we'll to group by\r\n",
        "  name     - name for the new features created\r\n",
        "  aggs     - dictionary contains key as the column the operation performed and list of operations as the value.\r\n",
        "  prefix   - added to the name of the feature -- default value empty\r\n",
        "  use_col  - if set True then the original column name will be uesd to name the new feature -- default value False\r\n",
        "  '''\r\n",
        "  ## boolean for using the original column name in the aggregated features\r\n",
        "  ## iterating through the columns of the need to be aggregated \r\n",
        "\r\n",
        "  for col, funcs in aggs.items():\r\n",
        "    for func in funcs:\r\n",
        "        # Getting the name of aggregation function\r\n",
        "        if isinstance(func, str):\r\n",
        "            func_str = func\r\n",
        "        else:\r\n",
        "            func_str = func.__name__ \r\n",
        "        # create the column\r\n",
        "        if use_col:\r\n",
        "          name = prefix+'_'+col+'_'+'{}'.format(func_str)\r\n",
        "\r\n",
        "        new_df[name] = df.groupby([grpby])[col].agg(func).values\r\n",
        "\r\n",
        "  return new_df "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN0rFarR46S_"
      },
      "source": [
        "def label_encoder(df, cols):\r\n",
        "  '''\r\n",
        "  This Function label encode the values in the specified columns and \r\n",
        "  return the data frame\r\n",
        "  Parameters:\r\n",
        "  df   - Original DataFrame\r\n",
        "  cols - label encode the specified columns\r\n",
        "  '''\r\n",
        "  lbl_enc = LabelEncoder()\r\n",
        "  for col in cols:\r\n",
        "    df[col] = lbl_enc.fit_transform(df[col].astype(str))\r\n",
        "  return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ub0Pdvxiyo"
      },
      "source": [
        "def category_3_encode(df):\r\n",
        "\r\n",
        "    ## label encode the category 3 variables.\r\n",
        "  d = {'A':1, 'B':2, 'C':3}\r\n",
        "  df['category_3']= df['category_3'].map(d)\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2COeXoq7tTL"
      },
      "source": [
        "def encode_transactions(df):\r\n",
        "  '''This function is specially for encode the categorical values of \r\n",
        "  transactions data\r\n",
        "  parameters:\r\n",
        "  df: the Dataframe where the label encoding will performed on certain features\r\n",
        "  '''\r\n",
        "\r\n",
        "\r\n",
        "  #label encode the variables\r\n",
        "  df = label_encoder(df, ['authorized_flag','category_1'])\r\n",
        "  return df\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7GszyT673PK"
      },
      "source": [
        "## Reference: https://medium.com/towards-artificial-intelligence/handling-missing-data-for-advanced-machine-learning-b6eb89050357\r\n",
        "\r\n",
        "def imputation_models(df, non_categorical,model_names, filepath, format):\r\n",
        "  '''This function is an universal function for imputation using \r\n",
        "  ML models\r\n",
        "  parameters:\r\n",
        "  df : The DataFrame\r\n",
        "  non_categorical : List of non_categorical columns names which can't be used for prediction model\r\n",
        "  nan_features: List of feature names which contains nan_values\r\n",
        "  models: dict , Key: feature name , value: Model\r\n",
        "  model_names: dict, key: feature name, value: name to be saved\r\n",
        "  filepath: file path where the models are saved.\r\n",
        "  format: format of model to be saved like '.sav'.\r\n",
        "  '''\r\n",
        "\r\n",
        "  if df[df.keys()].isna().any().any():\r\n",
        "    ## collecting all the non-categorical features \r\n",
        "    a = df[non_categorical]\r\n",
        "\r\n",
        "    ## dropping all the above collected features\r\n",
        "    df.drop(non_categorical, axis=1, inplace=True)\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    #select only columns which doesn't have any null values.\r\n",
        "    no_nan = [c for c in df.columns if c not in model_names.keys()]\r\n",
        "\r\n",
        "    for feat in model_names.keys():\r\n",
        "\r\n",
        "      if df[feat].isna().any().any():\r\n",
        "\r\n",
        "        #test set by selecting only rows which are having null values\r\n",
        "        df_null = df[df[feat].isna()]\r\n",
        "\r\n",
        "        #train set by selecting rows which doesn't have any null values\r\n",
        "        df_train = df.dropna()\r\n",
        "\r\n",
        "        ##after getting the required features, \r\n",
        "        ##we have to build a model with no-null observations to predict the null observstions.\r\n",
        "\r\n",
        "        file_directory = filepath+model_names[feat]+'.'+format\r\n",
        "\r\n",
        "        clf = pickle.load(open(file_directory, 'rb'))\r\n",
        "          \r\n",
        "\r\n",
        "        #make prediction only for the rows with null value\r\n",
        "        df.loc[df[feat].isna(), feat] = clf.predict(df_null[no_nan])\r\n",
        "\r\n",
        "    df[non_categorical] = a[non_categorical]\r\n",
        "    del a\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hniBzMaV8GWK"
      },
      "source": [
        "def oneHotEncoding(df, features, original_df):\r\n",
        "  '''This function is for one-hot encoding the categorical features\r\n",
        "  parameters:\r\n",
        "  df: DataFrame\r\n",
        "  features: Features needs to be one hot encoded.'''\r\n",
        "\r\n",
        "  for feat in features:\r\n",
        "    unique_values = original_df[feat].unique()\r\n",
        "\r\n",
        "    for cat in unique_values:\r\n",
        "      df[feat+'={}'.format(cat)] = (df[feat] == cat).astype(int)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dnp9Avf89dq"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def get_monthlag_stat(new_df, df, grpby, op, col, name, prefix=''):\r\n",
        "  \r\n",
        "  '''\r\n",
        "  Thid function is group by the the specified column and find the count or sum depending on the input.\r\n",
        "  Then perform basic operations like std, min, max etcetera\r\n",
        "  parameters\r\n",
        "  new_df - new features will be added to this DF\r\n",
        "  df     - original DF\r\n",
        "  grpby  - column using which we will group the data by\r\n",
        "  col    - operations will be performed on this column\r\n",
        "  name   - name for this columnn\r\n",
        "  prefix - prefix to the column name\r\n",
        "  '''\r\n",
        "  if op == 'sum':\r\n",
        "    tmp = df.groupby(grpby)[col].sum().unstack()\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[0]] = tmp.reset_index().iloc[:, -1].values\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[1]] = tmp.reset_index().iloc[:, -2].values\r\n",
        "  \r\n",
        "  if op == 'count':\r\n",
        "    tmp = df.groupby(grpby)[col].count().unstack()\r\n",
        "    # check if there is any null value and fill it with 0\r\n",
        "    if tmp.isna().sum().any() > 0:\r\n",
        "      tmp = tmp.fillna(0.0)\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[0]] = tmp.reset_index().iloc[:, 1:].std(axis=1).values\r\n",
        "    new_df[prefix+grpby[1]+'_'+name[1]] = tmp.reset_index().iloc[:, 1:].max(axis=1).values\r\n",
        "    \r\n",
        "  return new_df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO8osQyF9D-K"
      },
      "source": [
        "#https://www.kaggle.com/fabiendaniel/elo-world?scriptVersionId=8335387\r\n",
        "def successive_aggregates(df, field1, field2):\r\n",
        "  '''\r\n",
        "  This function does is that it group the data twice and find\r\n",
        "  basic aggregate values.\r\n",
        "  First it will goup by card_id and all the specified column one by one.\r\n",
        "  Then it will find the agg values like mean, min, max and std\r\n",
        "  for the purchase amount for each group.\r\n",
        "  Parameters:\r\n",
        "  df      - original DataFrame\r\n",
        "  field1  - first groupby along with card_id\r\n",
        "  field2  - second grouby along with card_id\r\n",
        "  '''\r\n",
        "  t = df.groupby(['card_id', field1])[field2].mean()\r\n",
        "  u = pd.DataFrame(t).reset_index().groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std'])\r\n",
        "  u.columns = [field1 + '_' + field2 + '_' + col for col in u.columns.values]\r\n",
        "  u.reset_index(inplace=True)\r\n",
        "  return u\r\n",
        "\r\n",
        "\r\n",
        "def successive_aggregation(new_df, df, field1='', field2 = '', other_columns=[]):\r\n",
        "  '''\r\n",
        "  This function is aggregates transactions columns successively with the columns mentioned.\r\n",
        "\r\n",
        "  parameters:\r\n",
        "\r\n",
        "  new_df  - The DataFrame which need to merged at end.\r\n",
        "  df      - The DataFrame with all the column required.\r\n",
        "  field1  - This is first field of many successive fields.\r\n",
        "  field2 - The second field for the aggregation, which constant for which te aggregation done with other features.\r\n",
        "  other_columns - all the other columns which will be later aggregated as the field_1.\r\n",
        "  '''\r\n",
        "\r\n",
        "  ## temp df for storing the aggregations \r\n",
        "  succ_agg = successive_aggregates(df, field1= field1, field2= field2)\r\n",
        "  ## creating other successive features \r\n",
        "  for  col in other_columns:\r\n",
        "    succ_agg = succ_agg.merge(successive_aggregates(df, col, field2),on=['card_id'], how='left')\r\n",
        "  \r\n",
        "  ## merge with the new df finally\r\n",
        "  new_df = new_df.merge(succ_agg, on = ['card_id'], how = 'left')\r\n",
        "\r\n",
        "  return new_df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgNwDvey9OsK"
      },
      "source": [
        "def getdatefeatures(df):\r\n",
        "  '''This function is to get all the numerical \r\n",
        "  features from the purchase Date feature\r\n",
        "  Parameters:\r\n",
        "  df: The dataFrame'''\r\n",
        "\r\n",
        "  df['week'] = df['purchase_date'].dt.week.values\r\n",
        "  df['dayofweek'] = df['purchase_date'].dt.dayofweek.values\r\n",
        "  df['hour'] = df['purchase_date'].dt.hour.values\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uupi2XpV9pGM"
      },
      "source": [
        "def timebtwpurchases(df, groupby, column, shift = 1):\r\n",
        "\r\n",
        "  ''' This function is to extract the shifted columns between the purchases.\r\n",
        "\r\n",
        "  parameters:\r\n",
        "\r\n",
        "  df - The DataFrame\r\n",
        "  groupby - The col by which the data is grouped by\r\n",
        "  column - column for which the shifted features are calcuated\r\n",
        "  shift - how many time periods shift is required\r\n",
        "  '''\r\n",
        "\r\n",
        "\r\n",
        "  ## first to sort values by the column for which shifting needed.\r\n",
        "\r\n",
        "  df = df.sort_values(column)\r\n",
        "\r\n",
        "  for i in range(shift):\r\n",
        "    ## creating the shited time between two purchases in a group\r\n",
        "    df['prev_{}_'.format(i+1)+column] = df.groupby([groupby])[column].shift(i+1)\r\n",
        "    ## calcuating the shift in different time representations like days,seconds,hours\r\n",
        "    df['purchase_date_diff_{}_days'.format(i+1)] = (df[column] - df['prev_{}_'.format(i+1)+column]).dt.days.values\r\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] = df['purchase_date_diff_{}_days'.format(i+1)].values * 24 * 3600\r\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] += (df[column] - df['prev_{}_'.format(i+1)+column]).dt.seconds.values\r\n",
        "    df['purchase_date_diff_{}_hours'.format(i+1)] = df.iloc[:, -1].values // 3600\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VazU-dCh9uAR"
      },
      "source": [
        "## reference: https://towardsdatascience.com/find-your-best-customers-with-customer-segmentation-in-python-61d602f9eee6\r\n",
        "def RScore(x,p,d):\r\n",
        "    if x <= d[p][0.011]:\r\n",
        "        return 1\r\n",
        "    elif x <= d[p][0.050]:\r\n",
        "        return 2\r\n",
        "    elif x <= d[p][0.25]: \r\n",
        "        return 3\r\n",
        "    elif x <= d[p][0.5]:\r\n",
        "        return 4\r\n",
        "    elif x <= d[p][0.75]:\r\n",
        "        return 5\r\n",
        "    elif x <= d[p][0.95]:\r\n",
        "        return 6\r\n",
        "    elif x <= d[p][0.989]:\r\n",
        "        return 7\r\n",
        "    else:\r\n",
        "        return 8\r\n",
        "    \r\n",
        "def FMScore(x,p,d):\r\n",
        "    if x <= d[p][0.011]:\r\n",
        "        return 8\r\n",
        "    elif x <= d[p][0.050]:\r\n",
        "        return 7\r\n",
        "    elif x <= d[p][0.25]: \r\n",
        "        return 6\r\n",
        "    elif x <= d[p][0.5]:\r\n",
        "        return 5\r\n",
        "    elif x <= d[p][0.75]:\r\n",
        "        return 4\r\n",
        "    elif x <= d[p][0.95]:\r\n",
        "        return 3\r\n",
        "    elif x <= d[p][0.989]:\r\n",
        "        return 2\r\n",
        "    else:\r\n",
        "        return 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFyZXv8A-QUh"
      },
      "source": [
        "def rfm(df,quantiles, transc):\r\n",
        "  '''This function is to calcualte RFM score and RFM index for the dataframe with RFM.\r\n",
        "\r\n",
        "  parameters:\r\n",
        "\r\n",
        "  df: The DataFrame\r\n",
        "  quantiles: qunatiles for the RFM score and index to calculated\r\n",
        "  transc : type of transactions(new or hist)\r\n",
        "\r\n",
        "  '''\r\n",
        "\r\n",
        "  ## grouping quantiles\r\n",
        "  df[transc+'r_quantile'] = df[transc+'purchase_recency'].apply(RScore, args=(transc+'purchase_recency',quantiles))\r\n",
        "  df[transc+'f_quantile'] = df[transc+'count'].apply(FMScore, args=(transc+'count',quantiles))\r\n",
        "  df[transc+'m_quantile'] = df[transc+'purchase_amount_sum'].apply(FMScore, args=(transc+'purchase_amount_sum',quantiles))\r\n",
        "  ## calaculating RFM index and RFMScore\r\n",
        "  df[transc+'RFMindex'] = df[transc+'r_quantile'].map(str)+df[transc+'f_quantile'].map(str)+df[transc+'m_quantile'].map(str)\r\n",
        "  df[transc+'RFMindex'] = df[transc+'RFMindex'].astype(int)                     \r\n",
        "  df[transc+'RFMScore'] = df[transc+'r_quantile']+df[transc+'f_quantile']+df[transc+'m_quantile'] "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z3LoWcM5Ppe"
      },
      "source": [
        "holidays = {'EasterDay_2017' : '2017-04-16',\r\n",
        "          'AllSoulsDay_2017': '2017-11-2',\r\n",
        "          'ChristmasDay_2017': '2017-12-25',\r\n",
        "          'FathersDay_2017': '2017-08-13',\r\n",
        "          'ChildrenDay_2017':'2017-10-12',\r\n",
        "          'BlackFriday_2017':'2017-11-24',\r\n",
        "          'ValentineDay_2017':'2017-06-12',\r\n",
        "          'MothersDay_2018':'2018-05-13'}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqRL9bI6hydM"
      },
      "source": [
        "def preprocess(new_merchant_transactions,historical_transactions):\r\n",
        "    \r\n",
        "\r\n",
        "  non_categorical= ['card_id', 'merchant_id', 'purchase_date']\r\n",
        "\r\n",
        "\r\n",
        "  model_names_transacations= {'category_2':'category_2_new_merchants_model',\r\n",
        "                        'category_3':'category_3_new_merchants_model' }\r\n",
        "\r\n",
        "\r\n",
        "  ## encoding the categorical features in new_merchants\r\n",
        "  new_merchant_transactions = encode_transactions(new_merchant_transactions)\r\n",
        "\r\n",
        "  new_merchant_transactions = imputation_models(df = new_merchant_transactions,\r\n",
        "                    non_categorical = non_categorical,\r\n",
        "                    model_names =model_names_transacations,\r\n",
        "                    filepath = master_path,\r\n",
        "                    format = '.sav')\r\n",
        "  \r\n",
        "\r\n",
        "  model_names_transacations= {'category_2':'category_2_historical_merchants_model',\r\n",
        "                      'category_3':'category_3_historical_merchants_model' }\r\n",
        "\r\n",
        "\r\n",
        "  ## encoding the categorical features in historical transactions\r\n",
        "  historical_transactions = encode_transactions(historical_transactions)\r\n",
        "\r\n",
        "  historical_transactions = imputation_models(df = historical_transactions,\r\n",
        "                    non_categorical = non_categorical,\r\n",
        "                    model_names =model_names_transacations,\r\n",
        "                    filepath = master_path,\r\n",
        "                    format = '.sav')\r\n",
        "  \r\n",
        "  \r\n",
        "  ## One-hot encoding the categorical features\r\n",
        "  categorical_features = ['category_2','category_3','month_lag']\r\n",
        "\r\n",
        "  ## one-hot encoding historical transactions \r\n",
        "  oneHotEncoding(historical_transactions, features=categorical_features,\r\n",
        "                original_df = historical_transactions_df)\r\n",
        "\r\n",
        "  ## one-hot encoding new merchants transactions\r\n",
        "  oneHotEncoding(new_merchant_transactions, features=categorical_features,\r\n",
        "                original_df = new_merchant_transactions_df)\r\n",
        "  \r\n",
        "\r\n",
        "  ## preprocess the purchase_amount\r\n",
        "  new_merchant_transactions['purchase_amount'] = np.round(new_merchant_transactions['purchase_amount'] / 0.00150265118 + 497.06, 2)\r\n",
        "  historical_transactions['purchase_amount'] = np.round(historical_transactions['purchase_amount'] / 0.00150265118 + 497.06, 2)\r\n",
        "\r\n",
        "  #is_weekend is a feature which purchase_date is weekend or weekday.\r\n",
        "  new_merchant_transactions['is_weekend'] = new_merchant_transactions['purchase_date'].dt.dayofweek\r\n",
        "  #>5 to check whether the day is sat or sunday then, if it is then assign a val 1 else 0\r\n",
        "  new_merchant_transactions['is_weekend'] = new_merchant_transactions['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\r\n",
        "  historical_transactions['is_weekend'] = historical_transactions['purchase_date'].dt.dayofweek\r\n",
        "  #>5 to check whether the day is sat or sunday then, if it is then assign a val 1 else 0\r\n",
        "  historical_transactions['is_weekend'] = historical_transactions['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\r\n",
        "\r\n",
        "  reference_date = '2018-12-31'\r\n",
        "  reference_date = pd.to_datetime(reference_date)\r\n",
        "  ## calcuating month difference \r\n",
        "  new_merchant_transactions['month_diff'] = (reference_date - new_merchant_transactions['purchase_date']).dt.days // (30 + new_merchant_transactions['month_lag'])\r\n",
        "  historical_transactions['month_diff'] = (reference_date - historical_transactions['purchase_date']).dt.days // (30 + historical_transactions['month_lag'])\r\n",
        "\r\n",
        "  new_merchant_transactions['amount_month_ratio'] = new_merchant_transactions['purchase_amount'].values / (1.0 + new_merchant_transactions['month_diff'].values)\r\n",
        "  historical_transactions['amount_month_ratio'] = historical_transactions['purchase_amount'].values / (1.0 + historical_transactions['month_diff'].values)\r\n",
        "\r\n",
        "  getdatefeatures(historical_transactions)\r\n",
        "  getdatefeatures(new_merchant_transactions)\r\n",
        "\r\n",
        "  new_merchant_transactions = timebtwpurchases(new_merchant_transactions, 'card_id', 'purchase_date', 2)\r\n",
        "\r\n",
        "\r\n",
        "  ## we are gonna represent number days as the feature. if the values is above 75 then it will become zero.\r\n",
        "  for day, date in holidays.items():\r\n",
        "    ## new_transactions \r\n",
        "    new_merchant_transactions[day] = (pd.to_datetime(date) - new_merchant_transactions['purchase_date']).dt.days\r\n",
        "    new_merchant_transactions[day] = new_merchant_transactions[day].apply(lambda x: x if x > 0 and x < 75 else 0)\r\n",
        "\r\n",
        "\r\n",
        "  return new_merchant_transactions,historical_transactions"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV9x0kyt8Kek"
      },
      "source": [
        "def engineered_features(new,ht):\r\n",
        "  \r\n",
        "  ## new_transactions\r\n",
        "  new_merch_features = pd.DataFrame(new.groupby(['card_id']).size()).reset_index()\r\n",
        "  new_merch_features.columns = ['card_id', 'new_transc_count']\r\n",
        "  ## historical_transactions \r\n",
        "  historical_trans_features = pd.DataFrame(ht.groupby(['card_id']).size()).reset_index()\r\n",
        "  historical_trans_features.columns = ['card_id', 'hist_transc_count']\r\n",
        "\r\n",
        "\r\n",
        "  ##unique id's in the transactions\r\n",
        "  aggs = {'city_id':['nunique'],\r\n",
        "        'state_id' :['nunique'],\r\n",
        "        'merchant_category_id':['nunique'],\r\n",
        "        'subsector_id':['nunique'],\r\n",
        "        'merchant_id':['nunique']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "  \r\n",
        "  ## categorty enigneered features\r\n",
        "  aggs = {'category_1':['sum', 'mean'],\r\n",
        "          'authorized_flag': ['sum', 'mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "    \r\n",
        "  ## category_1\r\n",
        "  new_merch_features['new_transc_category_1_sum_0'] = new_merch_features['new_transc_count'].values - \\\r\n",
        "  new_merch_features['new_transc_category_1_sum'].values\r\n",
        "  historical_trans_features['hist_transc_category_1_sum_0'] = historical_trans_features['hist_transc_count'].values - \\\r\n",
        "  historical_trans_features['hist_transc_category_1_sum'].values\r\n",
        "  ## authorized_flag\r\n",
        "  new_merch_features['new_transc_denied_count'] = new_merch_features['new_transc_count'].values - \\\r\n",
        "  new_merch_features['new_transc_authorized_flag_sum'].values\r\n",
        "  historical_trans_features['hist_transc_denied_count'] = historical_trans_features['hist_transc_count'].values - \\\r\n",
        "  historical_trans_features['hist_transc_authorized_flag_sum'].values\r\n",
        "\r\n",
        "\r\n",
        "  ### installment features \r\n",
        "  aggs = {'installments':['mean', 'sum', 'max', 'min', 'std', 'skew']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ### category_2 one-hot encoded features \r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'category_2=1.0':['sum', 'mean'],\r\n",
        "          'category_2=3.0':['sum', 'mean'],\r\n",
        "          'category_2=2.0':['sum', 'mean'],\r\n",
        "          'category_2=4.0':['sum', 'mean'],\r\n",
        "          'category_2=5.0':['sum', 'mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ### category_3 one-hot encoded features \r\n",
        "\r\n",
        "  aggs = {'category_3=2.0':['sum', 'mean'],\r\n",
        "          'category_3=1.0':['sum', 'mean'],\r\n",
        "          'category_3=3.0':['sum', 'mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  #find mean of the count of the transac for merchant id\r\n",
        "  historical_trans_features['hist_transc_merchant_id_count_mean'] = historical_trans_features['hist_transc_count'].values / (1.0+historical_trans_features['hist_transc_merchant_id_nunique'].values)\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features['new_transc_merchant_id_count_mean'] = new_merch_features['new_transc_count'].values/ (1.0+ new_merch_features['new_transc_merchant_id_nunique'].values)\r\n",
        "\r\n",
        "\r\n",
        "  grpby_lag = ['card_id', 'month_lag']\r\n",
        "  historical_trans_features = get_monthlag_stat(historical_trans_features, ht, grpby=grpby_lag, op='count',\r\n",
        "                                            col='purchase_amount', prefix='hist_transc_', name=['count_std','count_max'])\r\n",
        "\r\n",
        "  new_merch_features = get_monthlag_stat(new_merch_features, new, grpby=grpby_lag, op='count',\r\n",
        "                                        col='purchase_amount', prefix='new_transc_', name=['count_std','count_max'])\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'purchase_amount':['sum', 'mean', 'max', 'min', 'median', 'std', 'skew']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  ## difference in the amount spend with cards\r\n",
        "  new_merch_features['new_transc_amount_diff'] = new_merch_features['new_transc_purchase_amount_max'].values - new_merch_features['new_transc_purchase_amount_min'].values\r\n",
        "\r\n",
        "  historical_trans_features['hist_transc_amount_diff'] = historical_trans_features['hist_transc_purchase_amount_max'].values - historical_trans_features['hist_transc_purchase_amount_min'].values\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features = successive_aggregation(new_merch_features, new,\r\n",
        "                                              field1='category_1', field2 = 'purchase_amount',\r\n",
        "                                              other_columns = ['installments', 'city_id', \r\n",
        "                                                              'merchant_category_id', 'merchant_id',\r\n",
        "                                                              'subsector_id','category_2','category_3'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  historical_trans_features = successive_aggregation(historical_trans_features, ht,\r\n",
        "                                              field1='category_1', field2 = 'purchase_amount',\r\n",
        "                                              other_columns = ['installments', 'city_id', \r\n",
        "                                                              'merchant_category_id', 'merchant_id',\r\n",
        "                                                              'subsector_id','category_2','category_3'])\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'month_lag': ['nunique', 'mean', 'std', 'min', 'max', 'skew']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'purchase_date': ['max','min']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ## diff in purchase_date from max to min \r\n",
        "  new_merch_features['new_transc_purchase_date_diff'] = (new_merch_features['new_transc_purchase_date_max'] - new_merch_features['new_transc_purchase_date_min']).dt.days.values\r\n",
        "\r\n",
        "  ## purchase_count_ratio\r\n",
        "  new_merch_features['new_transc_purchase_count_ratio'] = new_merch_features['new_transc_count'].values / (1.0 + new_merch_features['new_transc_purchase_date_diff'].values)\r\n",
        "\r\n",
        "  ## diff in purchase_date from max to min \r\n",
        "  historical_trans_features['hist_transc_purchase_date_diff'] = (historical_trans_features['hist_transc_purchase_date_max'] - historical_trans_features['hist_transc_purchase_date_min']).dt.days.values\r\n",
        "\r\n",
        "  ## purchase_count_ratio\r\n",
        "  historical_trans_features['hist_transc_purchase_count_ratio'] = historical_trans_features['hist_transc_count'].values / (1.0 + historical_trans_features['hist_transc_purchase_date_diff'].values)\r\n",
        "\r\n",
        "\r\n",
        "  ## aggregate features for is_weekend \r\n",
        "  aggs = {'is_weekend': ['sum','mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  aggs = {'month_diff': ['mean', 'min', 'max']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  ## aggregated features on the amount ratio and month_lag.\r\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\r\n",
        "          'month_lag=1': ['sum','mean'],\r\n",
        "          'month_lag=2':['sum','mean']}\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\r\n",
        "          'month_lag=0': ['sum','mean'],\r\n",
        "          'month_lag=-1':['sum','mean'],\r\n",
        "          'month_lag=-2':['sum','mean']}\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  ## month_lag ratio between two month_lags.\r\n",
        "  new_merch_features['new_transc_month_lag_1_2_ratio'] = new_merch_features['new_transc_month_lag=1_sum'] \\\r\n",
        "                                                                    / (1.0+ new_merch_features['new_transc_month_lag=2_sum'])\r\n",
        "\r\n",
        "\r\n",
        "  ## month_lag ratio in historical transactions\r\n",
        "\r\n",
        "  historical_trans_features['hist_transc_month_lag_0_-1_ratio'] = historical_trans_features['hist_transc_month_lag=0_sum'] \\\r\n",
        "                                                                    / (1.0+ historical_trans_features['hist_transc_month_lag=-1_sum'])\r\n",
        "\r\n",
        "  historical_trans_features['hist_transc_month_lag_0_-2_ratio'] = historical_trans_features['hist_transc_month_lag=0_sum'] \\\r\n",
        "                                                                    / (1.0+ historical_trans_features['hist_transc_month_lag=-2_sum'])\r\n",
        "  tmp = historical_trans_features[['hist_transc_month_lag=0_sum','hist_transc_month_lag=-1_sum','hist_transc_month_lag=-2_sum']].sum(axis=1)\r\n",
        "\r\n",
        "  ## ratio of the summed month lags with the transaction\r\n",
        "  historical_trans_features['hist_transc_month_lag_sum_ratio'] = tmp / (1.0+ historical_trans_features['hist_transc_count'])\r\n",
        "\r\n",
        "\r\n",
        "  ## aggregated features on  day, hour , week\r\n",
        "  aggs = {'week': ['nunique', 'mean', 'min', 'max'],\r\n",
        "          'dayofweek': ['nunique', 'mean', 'min', 'max'],\r\n",
        "          'hour':['nunique', 'mean', 'min', 'max']}\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "  historical_trans_features = aggregated_features(historical_trans_features, ht,\r\n",
        "                                                  aggs, grpby='card_id',prefix='hist_transc', use_col=True)\r\n",
        "\r\n",
        "  ## aggregation difference in time features\r\n",
        "  aggs = {'purchase_date_diff_1_seconds': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_1_days': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_1_hours': ['mean', 'std', 'max', 'min']}\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "  ## aggregation difference in time features\r\n",
        "  aggs = {'purchase_date_diff_2_seconds': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_2_days': ['mean', 'std', 'max', 'min'],\r\n",
        "          'purchase_date_diff_2_hours': ['mean', 'std', 'max', 'min']}\r\n",
        "\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "    ## aggregation of holidays\r\n",
        "  aggs = dict(zip(holidays.keys(),[['mean'] for x in holidays.keys()]))\r\n",
        "\r\n",
        "  new_merch_features = aggregated_features(new_merch_features, new,\r\n",
        "                                          aggs, grpby='card_id',prefix='new_transc', use_col=True)\r\n",
        "\r\n",
        "\r\n",
        "  return new_merch_features,historical_trans_features\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZK2sAygxYfK"
      },
      "source": [
        "def preprocess_train(df,new_merch_features,historical_trans_features):\r\n",
        "\r\n",
        "  df = reduce(lambda left,right: pd.merge(left,right,on='card_id', how='left'), [df,new_merch_features,\r\n",
        "                                                                                 historical_trans_features])\r\n",
        "  \r\n",
        "  reference_date = pd.to_datetime('2018-12-31')\r\n",
        "  df['year'] = df['first_active_month'].dt.year.values\r\n",
        "  df['month'] = df['first_active_month'].dt.month.values\r\n",
        "  ## extracting elapsed dates \r\n",
        "  df['hist_transc_no_of_days'] = ( pd.to_datetime(df['hist_transc_purchase_date_max']) -  pd.to_datetime(df['hist_transc_purchase_date_max'])).dt.days\r\n",
        "  df['new_transc_no_of_days'] = (pd.to_datetime(df['new_transc_purchase_date_max']) - pd.to_datetime(df['new_transc_purchase_date_max'])).dt.days\r\n",
        "  ## recency of the puchases in terms of fractions\r\n",
        "  df['hist_transc_purchase_active_diff'] = (pd.to_datetime(df['hist_transc_purchase_date_min'].astype(str).apply(lambda x: x[:7])) - df['first_active_month']).dt.days.values\r\n",
        "  df['hist_transc_purchase_recency'] = (reference_date - pd.to_datetime(df['hist_transc_purchase_date_max']))/(24*np.timedelta64(1, 'h'))\r\n",
        "  df['new_transc_purchase_recency'] = (reference_date - pd.to_datetime(df['new_transc_purchase_date_max']))/(24*np.timedelta64(1, 'h')) \r\n",
        "\r\n",
        "  df = label_encoder(df, cols = ['month','year'])\r\n",
        "\r\n",
        "  quantiles_new = df[['new_transc_purchase_recency','new_transc_count','new_transc_purchase_amount_sum']].quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989]).to_dict()\r\n",
        "  ## quantiles of RFM with historical transactions\r\n",
        "  quantiles_hist = df[['hist_transc_purchase_recency','hist_transc_count','hist_transc_purchase_amount_sum']].quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989]).to_dict()\r\n",
        "\r\n",
        "  rfm(df,quantiles_new,transc = 'new_transc_')\r\n",
        "  rfm(df,quantiles_hist,transc = 'hist_transc_')\r\n",
        "\r\n",
        "  remove_cols = ['first_active_month','new_transc_purchase_date_max',\r\n",
        " 'new_transc_purchase_date_min','hist_transc_purchase_date_max',\r\n",
        " 'hist_transc_purchase_date_min']\r\n",
        "\r\n",
        "  df = df.drop(labels=remove_cols, axis = 1)\r\n",
        "\r\n",
        "  return df\r\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToRcpEDikkVD"
      },
      "source": [
        "def predict_loyalty_score(X):\r\n",
        "\r\n",
        "  ''' This function is predict the given card_id's predicted loyalty Score\r\n",
        "\r\n",
        "  parameters:\r\n",
        "  X : List of the card_id's\r\n",
        "\r\n",
        "  returns Dataframe with predicted loyalty score for each card_id.'''\r\n",
        "  \r\n",
        "\r\n",
        "  ## training the data \r\n",
        "\r\n",
        "  print(\"Fetching the transactional and card_id data\")\r\n",
        "  train = cards.loc[cards['card_id'].isin(X)]\r\n",
        "\r\n",
        "  historical_transactions = historical_transactions_df[historical_transactions_df['card_id'].isin(X)]\r\n",
        "  new_merchant_transactions = new_merchant_transactions_df[new_merchant_transactions_df['card_id'].isin(X)]\r\n",
        "  print(\"PreProcess the transactions data......\")\r\n",
        "  new_merchant_transactions,historical_transactions  = preprocess(new_merchant_transactions,historical_transactions)\r\n",
        "  print(\"Feature Engineering the transactions data.....\")\r\n",
        "  new_merch_features,historical_trans_features = engineered_features(new_merchant_transactions,historical_transactions)\r\n",
        "  print('preprocess the Feature Engineered Data')\r\n",
        "  train = preprocess_train(train,new_merch_features,historical_trans_features)\r\n",
        "\r\n",
        "  train.set_index('card_id',inplace=True)\r\n",
        "\r\n",
        "  print(\"Predicting the Loyalty Score.....\")\r\n",
        "  with open(master_path+'lgbm_final.sav', 'rb') as pickle_file:\r\n",
        "    mod = pickle.load(pickle_file)\r\n",
        "  pred_lgb = mod.predict(train , num_iteration=mod.best_iteration)\r\n",
        "\r\n",
        "  with open(master_path+'xgboost_final.sav', 'rb') as pickle_file:\r\n",
        "      mod = pickle.load(pickle_file)\r\n",
        "  pred_xgb = mod.predict(xgb.DMatrix(train[mod.feature_names]), ntree_limit=mod.best_ntree_limit+50)\r\n",
        "\r\n",
        "  meta_data = np.vstack([pred_xgb, pred_lgb]).transpose()\r\n",
        "\r\n",
        "  with open(master_path+'stacked_final.sav', 'rb') as pickle_file:\r\n",
        "      mod = pickle.load(pickle_file)\r\n",
        "  pred_stack = mod.predict(meta_data)\r\n",
        "\r\n",
        "  predcited_target = pd.DataFrame()\r\n",
        "\r\n",
        "  predcited_target['card_id'] = train.index \r\n",
        "\r\n",
        "  predcited_target['predicted_target'] = pred_stack\r\n",
        "\r\n",
        "  predcited_target.set_index('card_id',inplace=True)\r\n",
        "\r\n",
        "  return predcited_target"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6R52vL7KAs-"
      },
      "source": [
        "def rmse_score(X , target):\r\n",
        "\r\n",
        "  pred = predict_loyalty_score(X)\r\n",
        "  ## calculating the RMSE score\r\n",
        "  score = np.sqrt(mean_squared_error(pred, target))\r\n",
        "  print('RMSE Score:', score)\r\n",
        "\r\n",
        "  return score\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtXnoC-rFc0X"
      },
      "source": [
        "## Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNJ0Ix7LvkJS",
        "outputId": "4169ebf5-31d6-4453-d7c3-34f91d921485"
      },
      "source": [
        "## downloading the data \r\n",
        "downloadData()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/3.02M [00:00<?, ?B/s]\n",
            "100% 3.02M/3.02M [00:00<00:00, 48.6MB/s]\n",
            "Downloading Data_Dictionary.xlsx to /content\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 15.2MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/846k [00:00<?, ?B/s]\n",
            "100% 846k/846k [00:00<00:00, 110MB/s]\n",
            "Downloading Data%20Dictionary.xlsx to /content\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 14.4MB/s]\n",
            "Downloading historical_transactions.csv.zip to /content\n",
            " 98% 535M/548M [00:04<00:00, 143MB/s]\n",
            "100% 548M/548M [00:04<00:00, 132MB/s]\n",
            "Downloading new_merchant_transactions.csv.zip to /content\n",
            " 75% 37.0M/49.4M [00:00<00:00, 56.7MB/s]\n",
            "100% 49.4M/49.4M [00:00<00:00, 90.7MB/s]\n",
            "Downloading merchants.csv.zip to /content\n",
            " 71% 9.00M/12.7M [00:01<00:00, 11.2MB/s]\n",
            "100% 12.7M/12.7M [00:01<00:00, 10.9MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 160MB/s]\n",
            "Archive:  /content/historical_transactions.csv.zip\n",
            "  inflating: historical_transactions.csv  \n",
            "Archive:  /content/new_merchant_transactions.csv.zip\n",
            "  inflating: new_merchant_transactions.csv  \n",
            "Archive:  /content/merchants.csv.zip\n",
            "  inflating: merchants.csv           \n",
            "Archive:  /content/train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  /content/test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qSoivCSMWj1"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOX7Q_JZHMwN"
      },
      "source": [
        "To get the transactions data and merchants for the card_id for which we need to predcit the loyalty score, we are gonna load the data in the notebook. But in terms of deployment ofthe model we will get from a database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEvQVumMF5nX"
      },
      "source": [
        "train_df,historical_transactions_df,new_merchant_transactions_df,test_df = loadData()\r\n",
        "\r\n",
        "target  = train_df[['card_id','target']]\r\n",
        "target.set_index('card_id', inplace =True)\r\n",
        "\r\n",
        "cards = pd.concat([train_df.drop(['target'] , axis= 1) , test_df] , axis = 0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enZw6yurMKgd"
      },
      "source": [
        "## Predicting Loyalty Score for a Single Card_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MC7LpcVWzbW"
      },
      "source": [
        "X = train_df.sample(1)['card_id'].to_list()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "bGENYo06MFUr",
        "outputId": "1c40b5b7-141b-454d-d88f-feef3a28bd23"
      },
      "source": [
        "%%time\r\n",
        "predict_loyalty_score(X)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "CPU times: user 2.59 s, sys: 64.8 ms, total: 2.66 s\n",
            "Wall time: 6.09 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predicted_target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>card_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C_ID_0b92e83d26</th>\n",
              "      <td>-0.977876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 predicted_target\n",
              "card_id                          \n",
              "C_ID_0b92e83d26         -0.977876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rraluAPENHMm",
        "outputId": "d7bdd5ad-4b55-4849-f89a-77bf4341569c"
      },
      "source": [
        "%%time\r\n",
        "cal_rmse_score = rmse_score(X , target.loc[target.index.isin(X)])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "RMSE Score: 0.16609845708631488\n",
            "CPU times: user 1.96 s, sys: 21.5 ms, total: 1.98 s\n",
            "Wall time: 1.91 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WEn7dJ5V9xk"
      },
      "source": [
        "## Predicting Loyalty Score for a Set of train Card_id's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMoa7VEAWkAA"
      },
      "source": [
        "X = train_df.sample(1000)['card_id'].to_list()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "tjmkq-8aV9xn",
        "outputId": "e5dcdc67-2a10-4421-d6ca-a492e879c06c"
      },
      "source": [
        "%%time\r\n",
        "predict_loyalty_score(X)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "CPU times: user 5.47 s, sys: 154 ms, total: 5.62 s\n",
            "Wall time: 6.02 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predicted_target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>card_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C_ID_f478df16fd</th>\n",
              "      <td>-0.916580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_642f591402</th>\n",
              "      <td>0.312313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_028bd7dc53</th>\n",
              "      <td>-3.748715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_78bb01b56e</th>\n",
              "      <td>-3.138297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_4cfc17c2fb</th>\n",
              "      <td>0.691464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_86ee1869c8</th>\n",
              "      <td>-0.671134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_17d7a77690</th>\n",
              "      <td>0.390104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_16e43043a3</th>\n",
              "      <td>-0.208643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_2dca6b6d63</th>\n",
              "      <td>-1.614593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_4be8b0fed0</th>\n",
              "      <td>-1.017910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 predicted_target\n",
              "card_id                          \n",
              "C_ID_f478df16fd         -0.916580\n",
              "C_ID_642f591402          0.312313\n",
              "C_ID_028bd7dc53         -3.748715\n",
              "C_ID_78bb01b56e         -3.138297\n",
              "C_ID_4cfc17c2fb          0.691464\n",
              "...                           ...\n",
              "C_ID_86ee1869c8         -0.671134\n",
              "C_ID_17d7a77690          0.390104\n",
              "C_ID_16e43043a3         -0.208643\n",
              "C_ID_2dca6b6d63         -1.614593\n",
              "C_ID_4be8b0fed0         -1.017910\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a004orprV9xp",
        "outputId": "3a3ad14f-bfdb-4362-cb22-adc7994cf88d"
      },
      "source": [
        "%%time\r\n",
        "cal_rmse_score = rmse_score(X , target.loc[target.index.isin(X)])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "RMSE Score: 4.522284770915642\n",
            "CPU times: user 5.3 s, sys: 143 ms, total: 5.44 s\n",
            "Wall time: 5.12 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti7r1PREWSWN"
      },
      "source": [
        "## Predicting Loyalty Score for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_LpHW-rWeWp"
      },
      "source": [
        "X = test_df.sample(1)['card_id'].to_list()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "BQOTyqK7WSWO",
        "outputId": "a7d98db8-f680-4d64-b3fb-7a9865797abe"
      },
      "source": [
        "%%time\r\n",
        "predict_loyalty_score(X)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "CPU times: user 1.83 s, sys: 14.6 ms, total: 1.85 s\n",
            "Wall time: 1.77 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predicted_target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>card_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C_ID_c5fe25fbfd</th>\n",
              "      <td>-2.82573</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 predicted_target\n",
              "card_id                          \n",
              "C_ID_c5fe25fbfd          -2.82573"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CUr8NBYOw2"
      },
      "source": [
        "The Prediction time is low as 1.87 seconds. With, use of database to fetch the features of the transactions and card_id's the predcition time can be reduced more."
      ]
    }
  ]
}